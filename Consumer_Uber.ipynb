{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need some packages to talk to Kafka.\n",
    "# if we have no jars file it will automatic download\n",
    "# import os\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 pyspark-shell'\n",
    "\n",
    "# or set straight with spark configuration\n",
    "# spark = SparkSession.builder.appName('Streaming')\\\n",
    "#     .config('spark.jars.packages', 'org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2')\\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# create spark\n",
    "spark = SparkSession.builder.appName('Streaming')\\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read stream from kafka\n",
    "df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "      .option(\"subscribe\", \"DemoTopic\") \\\n",
    "      .option(\"startingOffsets\", \"earliest\") \\\n",
    "      .load()\n",
    "\n",
    "# .option(\"failOnDataLoss\", \"false\")\\\n",
    "# df_uber = df.select(col(\"key\").cast(\"string\"),from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\")).select(\"value.*\")\n",
    "# df_uber.printSchema()\n",
    "# df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") # convert key-value from binary to string type\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- lon: string (nullable = true)\n",
      " |-- base: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Parsing the messeage value into dataframe\n",
    "df_uber = df.select(col(\"value\").cast(\"string\")).alias(\"csv\").select(\"csv.*\")\n",
    "df_uber2 = df_uber.selectExpr(\"split(value,',')[0] as dt\",\n",
    "                               \"split(value,',')[1] as lat\",\n",
    "                               \"split(value,',')[2] as lon\",\n",
    "                               \"split(value,',')[3] as base\")\n",
    "df_uber2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show df\n",
    "# df_uber2.writeStream.format(\"console\").outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "fomart_datetime = time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "path = \"hdfs://localhost:9000/raws/raw_{}\".format(fomart_datetime)\n",
    "checkpoint = \"hdfs://localhost:9000/checkpoints/checkpoint-{}/\".format(fomart_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff1e0a1bfa0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write stream dataframe to hdfs\n",
    "df_uber2.writeStream.format(\"csv\") \\\n",
    "                    .option(\"path\", path) \\\n",
    "                    .option(\"checkpointLocation\", checkpoint) \\\n",
    "                    .outputMode(\"append\") \\\n",
    "                    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark.stop() \n",
    "# if using spark.stop() spark structure streaming will genarate \"Error in attempt 1 getting Kafka offsets:\" and does not write file to sink\n",
    "# if we using stop must be add .awaitterminate() to write stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8955aea2d0f270cb3725bdc50cc50d02d0e7b2461bf61a91c7963d76d191e8ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
