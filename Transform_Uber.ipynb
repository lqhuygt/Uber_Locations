{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create schema\n",
    "schema = StructType([ \n",
    "    StructField(\"dt\",TimestampType(), True), \n",
    "    StructField(\"lat\",DoubleType(), True), \n",
    "    StructField(\"lon\",DoubleType(), True), \n",
    "    StructField(\"base\", StringType(), True), \n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 10:51:27 WARN Utils: Your hostname, HuyLes-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.5 instead (on interface en0)\n",
      "22/04/12 10:51:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/huyle/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/huyle/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/huyle/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-979d334e-8186-4146-8fc2-f7c6b93b2509;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.2.14 in central\n",
      ":: resolution report :: resolve 199ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.postgresql#postgresql;42.2.14 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-979d334e-8186-4146-8fc2-f7c6b93b2509\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/8ms)\n",
      "22/04/12 10:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Uber')\\\n",
    "        .config('spark.jars.packages', 'org.postgresql:postgresql:42.2.14')\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+------+\n",
      "|                 dt|    lat|     lon|  base|\n",
      "+-------------------+-------+--------+------+\n",
      "|2014-08-01 00:00:00|40.7623|-73.9751|B02617|\n",
      "|2014-08-01 00:00:00|40.6982|-73.9669|B02617|\n",
      "|2014-08-01 00:00:00|40.7553|-73.9253|B02617|\n",
      "|2014-08-01 00:00:00|40.7325|-73.9876|B02682|\n",
      "|2014-08-01 00:00:00|40.6754| -74.017|B02682|\n",
      "+-------------------+-------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- base: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read data from hdfs \n",
    "# path = \"hdfs://localhost:9000/raws/raw_*/*\" # or \"hdfs://localhost:9000/raws/raw_*/*.csv\"\n",
    "path = \"hdfs://localhost:9000/raws/\" \n",
    "df_uber = spark.read.csv(path=path, schema=schema)\n",
    "\n",
    "# convert dt column to timestamp\n",
    "# df_uber = df.withColumn(\"dt\",to_timestamp(\"dt\").cast(\"timestamp\"))\n",
    "df_uber.show(5)\n",
    "df_uber.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "829359"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_uber.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+-------+------------------+\n",
      "|  dt|    lat|     lon|   base|          features|\n",
      "+----+-------+--------+-------+------------------+\n",
      "|null| 40.729|-73.9422|B02598\"| [40.729,-73.9422]|\n",
      "|null|40.7476|-73.9871|B02598\"|[40.7476,-73.9871]|\n",
      "|null|40.7424|-74.0044|B02598\"|[40.7424,-74.0044]|\n",
      "|null| 40.751|-73.9869|B02598\"| [40.751,-73.9869]|\n",
      "|null|40.7406|-73.9902|B02598\"|[40.7406,-73.9902]|\n",
      "+----+-------+--------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Denfine features vector to use for kmeans algorithm\n",
    "featureCols = ['lat', 'lon']\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol='features')\n",
    "\n",
    "df_uber2 = assembler.transform(df_uber)\n",
    "# df_uber2.cache()\n",
    "df_uber2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "0 [ 40.71570246 -73.95567131]\n",
      "1 [ 40.69812    -73.92200364]\n",
      "2 [ 40.69715 -73.58165]\n",
      "3 [ 40.86407143 -73.9237    ]\n",
      "4 [ 40.76353537 -73.96920884]\n",
      "5 [ 40.3495 -74.0667]\n",
      "6 [ 40.62990667 -73.96354   ]\n",
      "7 [ 40.740395   -74.00916056]\n",
      "8 [ 40.74973874 -73.98708333]\n",
      "9 [ 40.67906456 -73.9870962 ]\n",
      "10 [ 40.81074857 -73.95151429]\n",
      "11 [ 40.64623077 -73.78250769]\n",
      "12 [ 41.015  -73.6916]\n",
      "13 [ 40.72143617 -73.99729271]\n",
      "14 [ 41.05735 -74.14275]\n",
      "15 [ 40.74470625 -73.8257375 ]\n",
      "16 [ 40.75884667 -73.92368667]\n",
      "17 [ 41.0019 -74.0405]\n",
      "18 [ 40.7957 -74.4804]\n",
      "19 [ 40.78808 -74.12762]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# setK(20) phân thành 20 cụm\n",
    "# setFeaturesCol(\"features\") dùng để train\n",
    "# setPredictionCol(\"cid\") dùng để predict\n",
    "kmeans = KMeans().setK(20).setFeaturesCol(\"features\").setPredictionCol(\"cid\").setSeed(1)\n",
    "model = kmeans.fit(df_uber2)\n",
    "\n",
    "# Shows the result 20 cluster.\n",
    "centers = model.clusterCenters()\n",
    "i=0\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(i, center)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Silhouette score\n",
    "# evaluator = ClusteringEvaluator(predictionCol='cid', featuresCol='features',\n",
    "#                                 metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "\n",
    "# silhouette = evaluator.evaluate(predictions)\n",
    "# print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "# model.save(\"E:/PySpark/Uber_Locations/model/uber_location\")\n",
    "# model.write().overwrite().save(\"E:/PySpark/Uber_Locations/model/uber_location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+-------+------------------+---+\n",
      "|  dt|    lat|     lon|   base|          features|cid|\n",
      "+----+-------+--------+-------+------------------+---+\n",
      "|null| 40.729|-73.9422|B02598\"| [40.729,-73.9422]|  0|\n",
      "|null|40.7476|-73.9871|B02598\"|[40.7476,-73.9871]|  8|\n",
      "|null|40.7424|-74.0044|B02598\"|[40.7424,-74.0044]|  7|\n",
      "|null| 40.751|-73.9869|B02598\"| [40.751,-73.9869]|  8|\n",
      "|null|40.7406|-73.9902|B02598\"|[40.7406,-73.9902]|  8|\n",
      "+----+-------+--------+-------+------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make prediction\n",
    "df_predicted = model.transform(df_uber2)\n",
    "df_predicted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+-------+---+----------+\n",
      "|  dt|    lat|     lon|   base|cid|        id|\n",
      "+----+-------+--------+-------+---+----------+\n",
      "|null| 40.729|-73.9422|B02598\"|  0| 0_7299422|\n",
      "|null|40.7476|-73.9871|B02598\"|  8|8_74769871|\n",
      "|null|40.7424|-74.0044|B02598\"|  7|7_74240044|\n",
      "|null| 40.751|-73.9869|B02598\"|  8| 8_7519869|\n",
      "|null|40.7406|-73.9902|B02598\"|  8|8_74069902|\n",
      "+----+-------+--------+-------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, concat_ws, concat\n",
    "\n",
    "# add id column = cid + lat + lon\n",
    "split_lon = split(df_predicted.lon, \"\\.\").getItem(1)\n",
    "split_lat = split(df_predicted.lat, \"\\.\").getItem(1)\n",
    "id = concat(split_lat,split_lon) # nối chuỗi\n",
    "df_uber_id = df_predicted.withColumn(\"id\", concat_ws(\"_\",col(\"cid\"),id)) # add column \"id\"\n",
    "\n",
    "# drop feature column\n",
    "df_uber_locates = df_uber_id.drop(df_uber_id.features)\n",
    "df_uber_locates.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to hdfs   \n",
    "# df_uber_locates.write.mode(\"overwrite\").format(\"csv\") \\\n",
    "#                     .option(\"path\", \"hdfs://localhost:9000/transformed/\") \\\n",
    "#                     .save()\n",
    "\n",
    "# spark.stop()\n",
    "\n",
    "df_uber_locates.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/Test\")\\\n",
    "    .option(\"dbtable\", \"TestUber\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"Huy12345678\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95c10dbc6f7eccef0c1ace84822d618f7863d3bc26cab307fc0169bb43c23fbe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
