{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create schema\n",
    "schema = StructType([ \n",
    "    StructField(\"dt\",TimestampType(), True), \n",
    "    StructField(\"lat\",DoubleType(), True), \n",
    "    StructField(\"lon\",DoubleType(), True), \n",
    "    StructField(\"base\", StringType(), True), \n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Uber').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+------+\n",
      "|                 dt|    lat|     lon|  base|\n",
      "+-------------------+-------+--------+------+\n",
      "|2014-08-05 09:43:00| 40.726|-74.0013|B02682|\n",
      "|2014-08-31 07:29:00|40.7546|-73.9895|B02617|\n",
      "|2014-08-09 00:16:00|40.6768|-73.9801|B02598|\n",
      "|2014-08-17 19:30:00|40.7689|-73.9604|B02617|\n",
      "|2014-08-01 00:00:00| 40.729|-73.9422|B02598|\n",
      "+-------------------+-------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- base: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data from hdfs \n",
    "path = \"hdfs://localhost:9000/Uber_Warehouse/raw/\"\n",
    "df_uber = spark.read.csv(path=path, schema=schema)\n",
    "\n",
    "# convert dt column to timestamp\n",
    "# df_uber = df.withColumn(\"dt\",to_timestamp(\"dt\").cast(\"timestamp\"))\n",
    "df_uber.show(5)\n",
    "df_uber.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+------+------------------+\n",
      "|                 dt|    lat|     lon|  base|          features|\n",
      "+-------------------+-------+--------+------+------------------+\n",
      "|2014-08-05 09:43:00| 40.726|-74.0013|B02682| [40.726,-74.0013]|\n",
      "|2014-08-31 07:29:00|40.7546|-73.9895|B02617|[40.7546,-73.9895]|\n",
      "|2014-08-09 00:16:00|40.6768|-73.9801|B02598|[40.6768,-73.9801]|\n",
      "|2014-08-17 19:30:00|40.7689|-73.9604|B02617|[40.7689,-73.9604]|\n",
      "|2014-08-01 00:00:00| 40.729|-73.9422|B02598| [40.729,-73.9422]|\n",
      "+-------------------+-------+--------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Denfine features vector to use for kmeans algorithm\n",
    "featureCols = ['lat', 'lon']\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol='features')\n",
    "\n",
    "df_uber2 = assembler.transform(df_uber)\n",
    "df_uber2.cache()\n",
    "df_uber2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/01 13:52:11 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/02/01 13:52:11 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "[Stage 52:==========================================>           (156 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "0 [ 40.73423646 -73.99364081]\n",
      "1 [ 40.64987137 -73.78502466]\n",
      "2 [ 40.82184678 -73.94300184]\n",
      "3 [ 40.69717486 -74.18066285]\n",
      "4 [ 40.770255   -73.47119922]\n",
      "5 [ 40.75881196 -73.99109361]\n",
      "6 [ 40.66656304 -73.97881524]\n",
      "7 [ 40.7068365  -73.94677405]\n",
      "8 [ 40.67601567 -74.40629101]\n",
      "9 [ 40.75702524 -73.9217683 ]\n",
      "10 [ 40.77983533 -73.96266167]\n",
      "11 [ 40.71524847 -74.0061783 ]\n",
      "12 [ 40.20184004 -74.04641643]\n",
      "13 [ 40.75565536 -73.97356015]\n",
      "14 [ 41.02007523 -73.76031087]\n",
      "15 [ 40.74274103 -73.66676635]\n",
      "16 [ 40.75989546 -73.86056185]\n",
      "17 [ 40.91695269 -74.11372694]\n",
      "18 [ 40.80073204 -73.09776343]\n",
      "19 [ 40.87546667 -73.87859288]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# setK(20) phân thành 20 cụm\n",
    "# setFeaturesCol(\"features\") dùng để train\n",
    "# setPredictionCol(\"cid\") dùng để predict\n",
    "kmeans = KMeans().setK(20).setFeaturesCol(\"features\").setPredictionCol(\"cid\").setSeed(1)\n",
    "model = kmeans.fit(df_uber2)\n",
    "\n",
    "# Shows the result 20 cluster.\n",
    "centers = model.clusterCenters()\n",
    "i=0\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(i, center)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.5444916986154139\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering by computing Silhouette score\n",
    "# evaluator = ClusteringEvaluator(predictionCol='cid', featuresCol='features',\n",
    "#                                 metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "\n",
    "# silhouette = evaluator.evaluate(predictions)\n",
    "# print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "# model.save(\"E:/PySpark/Uber_Locations/model/uber_location\")\n",
    "# model.write().overwrite().save(\"E:/PySpark/Uber_Locations/model/uber_location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+------+------------------+---+\n",
      "|                 dt|    lat|     lon|  base|          features|cid|\n",
      "+-------------------+-------+--------+------+------------------+---+\n",
      "|2014-08-05 09:43:00| 40.726|-74.0013|B02682| [40.726,-74.0013]|  0|\n",
      "|2014-08-31 07:29:00|40.7546|-73.9895|B02617|[40.7546,-73.9895]|  5|\n",
      "|2014-08-09 00:16:00|40.6768|-73.9801|B02598|[40.6768,-73.9801]|  6|\n",
      "|2014-08-17 19:30:00|40.7689|-73.9604|B02617|[40.7689,-73.9604]| 10|\n",
      "|2014-08-01 00:00:00| 40.729|-73.9422|B02598| [40.729,-73.9422]|  7|\n",
      "+-------------------+-------+--------+------+------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make prediction\n",
    "df_predicted = model.transform(df_uber2)\n",
    "df_predicted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+------+---+-----------+\n",
      "|                 dt|    lat|     lon|  base|cid|         id|\n",
      "+-------------------+-------+--------+------+---+-----------+\n",
      "|2014-08-05 09:43:00| 40.726|-74.0013|B02682|  0|  0_7260013|\n",
      "|2014-08-31 07:29:00|40.7546|-73.9895|B02617|  5| 5_75469895|\n",
      "|2014-08-09 00:16:00|40.6768|-73.9801|B02598|  6| 6_67689801|\n",
      "|2014-08-17 19:30:00|40.7689|-73.9604|B02617| 10|10_76899604|\n",
      "|2014-08-01 00:00:00| 40.729|-73.9422|B02598|  7|  7_7299422|\n",
      "+-------------------+-------+--------+------+---+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, concat_ws, concat\n",
    "\n",
    "# add id column = cid + lat + lon\n",
    "split_lon = split(df_predicted.lon, \"\\.\").getItem(1)\n",
    "split_lat = split(df_predicted.lat, \"\\.\").getItem(1)\n",
    "id = concat(split_lat,split_lon) # nối chuỗi\n",
    "df_uber_id = df_predicted.withColumn(\"id\", concat_ws(\"_\",col(\"cid\"),id)) # add column \"id\"\n",
    "\n",
    "# drop feature column\n",
    "df_uber_locates = df_uber_id.drop(df_uber_id.features)\n",
    "df_uber_locates.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write to hdfs   \n",
    "df_uber_locates.write.mode(\"overwrite\").format(\"csv\") \\\n",
    "                    .option(\"path\", \"hdfs://localhost:9000/Uber_Warehouse/transformed/\") \\\n",
    "                    .option(\"checkpointLocation\", \"hdfs://localhost:9000/Uber_Warehouse/checkpoints/\") \\\n",
    "                    .save()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95c10dbc6f7eccef0c1ace84822d618f7863d3bc26cab307fc0169bb43c23fbe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
