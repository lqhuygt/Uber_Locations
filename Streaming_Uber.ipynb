{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "95c10dbc6f7eccef0c1ace84822d618f7863d3bc26cab307fc0169bb43c23fbe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Need some packages to talk to Kafka.\r\n",
    "import os\r\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 pyspark-shell'\r\n",
    "\r\n",
    "# from ast import literal_eval"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import findspark\r\n",
    "findspark.init()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "# create spark\r\n",
    "spark = SparkSession.builder.appName(\"Streaming\").getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# read stream from kafka\r\n",
    "df = spark\\\r\n",
    "      .readStream \\\r\n",
    "      .format(\"kafka\") \\\r\n",
    "      .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\r\n",
    "      .option(\"subscribe\", \"UberTopic\") \\\r\n",
    "      .option(\"startingOffsets\", \"earliest\") \\\r\n",
    "      .load()\r\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") # convert key-value from binary to string type\r\n",
    "df.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\r\n",
    "\r\n",
    "# Parsing the messeage value into dataframe\r\n",
    "df_uber = df.select(col(\"value\").cast(\"string\")).alias(\"csv\").select(\"csv.*\")\r\n",
    "df_uber2 = df_uber.selectExpr(\"split(value,',')[0] as dt\",\r\n",
    "                               \"split(value,',')[1] as lat\",\r\n",
    "                               \"split(value,',')[2] as lon\",\r\n",
    "                               \"split(value,',')[3] as base\")\r\n",
    "df_uber2.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- lon: string (nullable = true)\n",
      " |-- base: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# convert column type\r\n",
    "df_uber3 = df_uber2.withColumn(\"dt\",to_timestamp(\"dt\").cast(\"timestamp\")) \\\r\n",
    "                    .withColumn(\"lat\", col(\"lat\").cast(\"double\")) \\\r\n",
    "                    .withColumn(\"lon\", col(\"lon\").cast(\"double\")) \r\n",
    "df_uber3.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- base: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Show df\r\n",
    "# df_uber3.writeStream.queryName(\"Uber\").format(\"memory\").outputMode(\"append\").start()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x1e3b8b4e0d0>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# top5 = spark.sql(\"select * from Uber\")\r\n",
    "# top5.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-------+--------+------+\n",
      "|                 dt|    lat|     lon|  base|\n",
      "+-------------------+-------+--------+------+\n",
      "|2014-08-01 00:00:00| 40.729|-73.9422|B02598|\n",
      "|2014-08-01 00:00:00|40.7476|-73.9871|B02598|\n",
      "|2014-08-01 00:00:00|40.7424|-74.0044|B02598|\n",
      "|2014-08-01 00:00:00| 40.751|-73.9869|B02598|\n",
      "|2014-08-01 00:00:00|40.7406|-73.9902|B02598|\n",
      "+-------------------+-------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from pyspark.ml.feature import VectorAssembler\r\n",
    "# Denfine features vector to use for kmeans algorithm\r\n",
    "featureCols = ['lat', 'lon']\r\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol='features')\r\n",
    "\r\n",
    "df_uber4 = assembler.transform(df_uber3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# load model\r\n",
    "from pyspark.ml.clustering import KMeansModel\r\n",
    "model = KMeansModel.load(\"./model/uber_location\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# make prediction\r\n",
    "df_predicted = model.transform(df_uber4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# stream = df_predicted.writeStream.queryName(\"uber\").format(\"memory\").outputMode(\"append\").start()\r\n",
    "# display_trained = spark.sql(\"select * from uber\")\r\n",
    "# display_trained.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-------+--------+------+------------------+---+\n",
      "|                 dt|    lat|     lon|  base|          features|cid|\n",
      "+-------------------+-------+--------+------+------------------+---+\n",
      "|2014-08-01 00:00:00| 40.729|-73.9422|B02598| [40.729,-73.9422]|  2|\n",
      "|2014-08-01 00:00:00|40.7476|-73.9871|B02598|[40.7476,-73.9871]|  0|\n",
      "|2014-08-01 00:00:00|40.7424|-74.0044|B02598|[40.7424,-74.0044]|  0|\n",
      "|2014-08-01 00:00:00| 40.751|-73.9869|B02598| [40.751,-73.9869]| 13|\n",
      "|2014-08-01 00:00:00|40.7406|-73.9902|B02598|[40.7406,-73.9902]|  0|\n",
      "+-------------------+-------+--------+------+------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from pyspark.sql.functions import split, concat_ws, concat\r\n",
    "\r\n",
    "# add id column = cid + lat + lon\r\n",
    "split_lon = split(df_predicted.lon, \"\\.\").getItem(1)\r\n",
    "split_lat = split(df_predicted.lat, \"\\.\").getItem(1)\r\n",
    "id = concat(split_lat,split_lon) # nối chuỗi\r\n",
    "df_uber_id = df_predicted.withColumn(\"id\", concat_ws(\"_\",col(\"cid\"),id)) # add column \"id\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# drop feature column\r\n",
    "df_uber_locates = df_uber_id.drop(df_uber_id.features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# write stream dataframe to hdfs\r\n",
    "warehouse = df_uber_locates.writeStream.format(\"csv\") \\\r\n",
    "                                        .option(\"path\", \"hdfs://localhost:9000/Uber_Warehouse/data_warehouse\") \\\r\n",
    "                                        .option(\"checkpointLocation\", \"hdfs://localhost:9000/Uber_Warehouse/checkpoints\") \\\r\n",
    "                                        .outputMode(\"append\") \\\r\n",
    "                                        .start()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  producer = KafkaProducer(bootstrap_servers='mykafka-broker')\r\n",
    "#     with open('/home/antonis/repos/testfile.csv') as file:\r\n",
    "#         reader = csv.DictReader(file, delimiter=\";\")\r\n",
    "#         for row in reader:\r\n",
    "#             producer.send(topic='stable_topic', value=row)\r\n",
    "#             producer.flush()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}