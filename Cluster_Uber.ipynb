{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "95c10dbc6f7eccef0c1ace84822d618f7863d3bc26cab307fc0169bb43c23fbe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import findspark\r\n",
    "findspark.init()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField\r\n",
    "from pyspark.sql.functions import col, to_timestamp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# create schema\r\n",
    "schema = StructType([ \r\n",
    "    StructField(\"dt\",StringType(), True), \r\n",
    "    StructField(\"lat\",DoubleType(), True), \r\n",
    "    StructField(\"lon\",DoubleType(), True), \r\n",
    "    StructField(\"base\", StringType(), True), \r\n",
    "  ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# read data\r\n",
    "spark = SparkSession.builder.appName('Uber').getOrCreate()\r\n",
    "path = \"uber.csv\"\r\n",
    "df = spark.read.csv(path=path, schema=schema)\r\n",
    "\r\n",
    "# convert dt column to timestamp\r\n",
    "df_uber = df.withColumn(\"dt\",to_timestamp(\"dt\").cast(\"timestamp\"))\r\n",
    "df_uber.show(5)\r\n",
    "df_uber.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-------+--------+------+\n",
      "|                 dt|    lat|     lon|  base|\n",
      "+-------------------+-------+--------+------+\n",
      "|2014-08-01 00:00:00| 40.729|-73.9422|B02598|\n",
      "|2014-08-01 00:00:00|40.7476|-73.9871|B02598|\n",
      "|2014-08-01 00:00:00|40.7424|-74.0044|B02598|\n",
      "|2014-08-01 00:00:00| 40.751|-73.9869|B02598|\n",
      "|2014-08-01 00:00:00|40.7406|-73.9902|B02598|\n",
      "+-------------------+-------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- base: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from pyspark.ml.feature import VectorAssembler\r\n",
    "# Denfine features vector to use for kmeans algorithm\r\n",
    "featureCols = ['lat', 'lon']\r\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol='features')\r\n",
    "\r\n",
    "df_uber2 = assembler.transform(df_uber)\r\n",
    "df_uber2.cache()\r\n",
    "df_uber2.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-------+--------+------+------------------+\n",
      "|                 dt|    lat|     lon|  base|          features|\n",
      "+-------------------+-------+--------+------+------------------+\n",
      "|2014-08-01 00:00:00| 40.729|-73.9422|B02598| [40.729,-73.9422]|\n",
      "|2014-08-01 00:00:00|40.7476|-73.9871|B02598|[40.7476,-73.9871]|\n",
      "|2014-08-01 00:00:00|40.7424|-74.0044|B02598|[40.7424,-74.0044]|\n",
      "|2014-08-01 00:00:00| 40.751|-73.9869|B02598| [40.751,-73.9869]|\n",
      "|2014-08-01 00:00:00|40.7406|-73.9902|B02598|[40.7406,-73.9902]|\n",
      "+-------------------+-------+--------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from pyspark.ml.clustering import KMeans\r\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\r\n",
    "\r\n",
    "# setK(20) phân thành 20 cụm\r\n",
    "# setFeaturesCol(\"features\") dùng để train\r\n",
    "# setPredictionCol(\"cid\") dùng để predict\r\n",
    "kmeans = KMeans().setK(20).setFeaturesCol(\"features\").setPredictionCol(\"cid\").setSeed(1)\r\n",
    "model = kmeans.fit(df_uber2)\r\n",
    "\r\n",
    "# Shows the result 20 cluster.\r\n",
    "centers = model.clusterCenters()\r\n",
    "i=0\r\n",
    "print(\"Cluster Centers: \")\r\n",
    "for center in centers:\r\n",
    "    print(i, center)\r\n",
    "    i += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cluster Centers: \n",
      "0 [ 40.73909578 -73.99337027]\n",
      "1 [ 40.65009343 -73.78464196]\n",
      "2 [ 40.71824324 -73.9529467 ]\n",
      "3 [ 40.76011724 -73.86306385]\n",
      "4 [ 40.76949209 -73.95020413]\n",
      "5 [ 40.770255   -73.47119922]\n",
      "6 [ 40.73938927 -74.04208329]\n",
      "7 [ 40.68221995 -73.98402042]\n",
      "8 [ 40.62416966 -73.98202639]\n",
      "9 [ 40.20325862 -74.05279351]\n",
      "10 [ 40.71850996 -74.00190164]\n",
      "11 [ 40.69600569 -74.20238311]\n",
      "12 [ 41.00103971 -73.77328319]\n",
      "13 [ 40.75982169 -73.98040416]\n",
      "14 [ 40.68420017 -73.93498654]\n",
      "15 [ 40.80073204 -73.09776343]\n",
      "16 [ 40.79924197 -73.96137349]\n",
      "17 [ 40.92930041 -74.11616198]\n",
      "18 [ 40.74288204 -73.66693211]\n",
      "19 [ 40.85732825 -73.90904588]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Evaluate clustering by computing Silhouette score\r\n",
    "# evaluator = ClusteringEvaluator(predictionCol='cid', featuresCol='features',\r\n",
    "#                                 metricName='silhouette', distanceMeasure='squaredEuclidean')\r\n",
    "\r\n",
    "# silhouette = evaluator.evaluate(predictions)\r\n",
    "# print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Silhouette with squared euclidean distance = 0.5444916986154139\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "#save model\r\n",
    "try:\r\n",
    "    model.save(\"E:/PySpark/Uber_Locations/model/uber_location\")\r\n",
    "    # model.write().overwrite().save(\"E:/PySpark/Uber_Locations/model/uber_location\") # ghi đè\r\n",
    "    print(\"Save succes\")\r\n",
    "except:\r\n",
    "    print(\"Fail!\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}